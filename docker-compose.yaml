version: '3.8'

x-airflow-common: &airflow-common
  build:
    context: ./airflow
  user: root # Keep running as root to simplify permissions
  environment:
    - AIRFLOW__CORE__EXECUTOR=LocalExecutor
    - AIRFLOW__CORE__SQL_ALCHEMY_CONN=postgresql+psycopg2://airflow:airflow@postgres/airflow
    - AIRFLOW__CORE__LOAD_EXAMPLES=False
    - AWS_ACCESS_KEY_ID=${AWS_ACCESS_KEY_ID}
    - AWS_SECRET_ACCESS_KEY=${AWS_SECRET_ACCESS_KEY}
    - AWS_ACCOUNT_ID=${AWS_ACCOUNT_ID}
    - AWS_REGION=${AWS_REGION}
    - BRONZE_S3_BUCKET=${BRONZE_S3_BUCKET}
    - SILVER_S3_BUCKET=${SILVER_S3_BUCKET}
    - GOLD_S3_BUCKET=${GOLD_S3_BUCKET}
  volumes:
    - ./airflow/dags:/opt/airflow/dags
    - ./airflow/logs:/opt/airflow/logs
    - ./airflow/plugins:/opt/airflow/plugins
    - ./spark/jobs:/opt/spark/jobs
    - ./spark/conf:/opt/spark/conf # <<< ADDED HERE: Make hive-site.xml available to Airflow/Spark-Submit
    - ./data:/opt/data
    - spark-staging-area:/staging
    - ./jars:/opt/bitnami/spark/jars
    - ./scripts:/opt/airflow/scripts
  networks:
    - pipeline_net

services:

  mysql:
    image: mysql:8.0
    container_name: mysql_database
    restart: unless-stopped
    environment:
      MYSQL_ROOT_PASSWORD: ${MYSQL_ROOT_PASSWORD}
      MYSQL_DATABASE: sales # This matches your init.sql
      MYSQL_USER: ${MYSQL_USER}
      MYSQL_PASSWORD: ${MYSQL_PASSWORD}
    ports:
      - "3306:3306"
    volumes:
      - mysql_data:/var/lib/mysql
      - ./db:/docker-entrypoint-initdb.d
    networks:
      - pipeline_net
    healthcheck:
      test: [ "CMD", "mysqladmin", "ping", "-h", "localhost" ]
      timeout: 20s
      retries: 10
    command: --default-authentication-plugin=mysql_native_password

  postgres:
    image: postgres:13
    container_name: postgres
    ports:
      - "5432:5432"
    environment:
      - POSTGRES_USER=airflow
      - POSTGRES_PASSWORD=airflow
      - POSTGRES_DB=airflow
    volumes:
      - postgres-db-volume:/var/lib/postgresql/data
    networks:
      - pipeline_net

  airflow-init:
    <<: *airflow-common
    container_name: airflow_init
    depends_on:
      - postgres
    command: >
      bash -c "airflow db init && airflow users create --username admin --password admin --firstname Anonymous --lastname Admin --role Admin --email admin@example.org"

  airflow-webserver:
    <<: *airflow-common
    container_name: airflow_webserver
    restart: always
    depends_on:
      - airflow-init
    ports:
      - "8080:8080"
    command: webserver

  airflow-scheduler:
    <<: *airflow-common
    container_name: airflow_scheduler
    restart: always
    depends_on:
      - airflow-init
    command: scheduler

  api:
    build:
      context: ./api
      dockerfile: Dockerfile
    ports:
      - "8001:8000"
    volumes:
      - ./api:/app
    networks:
      - pipeline_net

  spark-master:
    build:
      context: ./airflow
    container_name: spark_master
    environment:
      - SPARK_CONF_fs_s3a_access_key=${AWS_ACCESS_KEY_ID}
      - SPARK_CONF_fs_s3a_secret_key=${AWS_SECRET_ACCESS_KEY}
      - SPARK_CONF_fs_s3a_endpoint=s3.${AWS_REGION}.amazonaws.com
      - SPARK_CONF_fs_s3a_aws_credentials_provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
    user: root
    ports:
      - "8081:8080"
      - "7077:7077"
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/conf:/opt/spark/conf # <<< ADDED HERE: Make hive-site.xml available to Spark Master
      - ./data:/opt/data
      - spark-staging-area:/staging
      - ./jars:/opt/bitnami/spark/jars
    networks:
      pipeline_net:
        aliases:
          - spark-master
    command: >
      bash -c "/opt/spark/sbin/start-master.sh && tail -f /dev/null"

  spark-worker:
    build:
      context: ./airflow
    container_name: spark_worker
    environment:
      - SPARK_CONF_fs_s3a_access_key=${AWS_ACCESS_KEY_ID}
      - SPARK_CONF_fs_s3a_secret_key=${AWS_SECRET_ACCESS_KEY}
      - SPARK_CONF_fs_s3a_endpoint=s3.${AWS_REGION}.amazonaws.com
      - SPARK_CONF_fs_s3a_aws_credentials_provider=org.apache.hadoop.fs.s3a.SimpleAWSCredentialsProvider
      - SPARK_CONF_spark_driver_extraClassPath=/root/.ivy2/jars/*
      - SPARK_CONF_spark_executor_extraClassPath=/root/.ivy2/jars/*
      - SPARK_CONF_spark_jars_ivy=/root/.ivy2
    user: root
    depends_on:
      - spark-master
    ports:
      - "8082:8080"
    volumes:
      - ./spark/jobs:/opt/spark/jobs
      - ./spark/conf:/opt/spark/conf # <<< ADDED HERE: Make hive-site.xml available to Spark Worker
      - ./data:/opt/data
      - spark-staging-area:/staging
      - ./jars:/opt/bitnami/spark/jars
    networks:
      - pipeline_net
    command: >
      bash -c "/opt/spark/sbin/start-worker.sh spark://spark-master:7077 && tail -f /dev/null"

volumes:
  postgres-db-volume:
  spark-staging-area:
  mysql_data:

networks:
  pipeline_net:
    driver: bridge